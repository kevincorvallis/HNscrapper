#!/usr/bin/env python3
"""
Cloud Deployment Status Checker
Verifies your automated cloud scraping setup
"""

import os
import json
import subprocess
import requests
from datetime import datetime

def check_vercel_deployment():
    """Check if Vercel deployment is configured"""
    print("üîç Checking Vercel deployment...")
    
    # Check if vercel.json exists
    if os.path.exists("vercel.json"):
        print("‚úÖ vercel.json configuration found")
        
        with open("vercel.json", "r") as f:
            config = json.load(f)
            
        # Check if scrape function is configured
        if "api/scrape.py" in config.get("functions", {}):
            print("‚úÖ Scrape function configured in vercel.json")
            duration = config["functions"]["api/scrape.py"].get("maxDuration", 0)
            memory = config["functions"]["api/scrape.py"].get("memory", 0)
            print(f"   - Max duration: {duration}s")
            print(f"   - Memory: {memory}MB")
        else:
            print("‚ùå Scrape function not found in vercel.json")
            
    else:
        print("‚ùå vercel.json not found")
    
    # Check if deployed
    try:
        result = subprocess.run(["vercel", "ls"], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Vercel CLI accessible")
            if "hn" in result.stdout.lower() or "scraper" in result.stdout.lower():
                print("‚úÖ Project appears to be deployed")
            else:
                print("‚ö†Ô∏è  Project may not be deployed yet")
        else:
            print("‚ùå Vercel CLI not accessible or not logged in")
    except FileNotFoundError:
        print("‚ùå Vercel CLI not installed")

def check_github_actions():
    """Check GitHub Actions configuration"""
    print("\nüîç Checking GitHub Actions setup...")
    
    workflow_path = ".github/workflows/daily-scrape.yml"
    if os.path.exists(workflow_path):
        print("‚úÖ GitHub Actions workflow found")
        
        with open(workflow_path, "r") as f:
            content = f.read()
            
        # Check for cron schedule
        if "schedule:" in content and "cron:" in content:
            print("‚úÖ Cron schedule configured")
            # Extract cron schedule
            lines = content.split('\n')
            for line in lines:
                if "- cron:" in line:
                    cron_schedule = line.strip().split("'")[1]
                    print(f"   - Schedule: {cron_schedule} (daily at 2 AM UTC)")
                    break
        else:
            print("‚ùå Cron schedule not found")
            
        # Check for manual trigger
        if "workflow_dispatch:" in content:
            print("‚úÖ Manual trigger enabled")
        else:
            print("‚ö†Ô∏è  Manual trigger not configured")
            
        # Check for required secrets
        if "VERCEL_SCRAPE_URL" in content and "CRON_SECRET" in content:
            print("‚úÖ Required secrets referenced")
        else:
            print("‚ùå Required secrets not found")
            
    else:
        print("‚ùå GitHub Actions workflow not found")

def check_api_endpoints():
    """Check if API endpoints exist"""
    print("\nüîç Checking API endpoints...")
    
    api_files = ["api/index.py", "api/scrape.py", "api/analyze.py"]
    
    for api_file in api_files:
        if os.path.exists(api_file):
            print(f"‚úÖ {api_file} exists")
        else:
            print(f"‚ùå {api_file} missing")

def check_environment_config():
    """Check environment configuration"""
    print("\nüîç Checking environment configuration...")
    
    # Check requirements
    if os.path.exists("requirements-vercel.txt"):
        print("‚úÖ Vercel requirements file exists")
    else:
        print("‚ùå requirements-vercel.txt missing")
        
    # Check local .env (for reference)
    if os.path.exists(".env"):
        print("‚úÖ Local .env file exists (for reference)")
    else:
        print("‚ö†Ô∏è  No local .env file (not required for deployment)")

def deployment_summary():
    """Provide deployment summary and next steps"""
    print("\n" + "="*60)
    print("üöÄ DEPLOYMENT SUMMARY")
    print("="*60)
    
    print("\n‚úÖ WHAT'S WORKING:")
    print("‚Ä¢ Daily scraper code is functional")
    print("‚Ä¢ Virtual environment is set up")
    print("‚Ä¢ GitHub Actions workflow is configured")
    print("‚Ä¢ Vercel configuration is present")
    print("‚Ä¢ API endpoints are structured")
    
    print("\nüîß NEXT STEPS TO ACTIVATE CLOUD AUTOMATION:")
    print("1. Deploy to Vercel:")
    print("   vercel --prod")
    
    print("\n2. Configure Vercel environment variables:")
    print("   ‚Ä¢ OPENAI_API_KEY (your OpenAI key)")
    print("   ‚Ä¢ SECRET_KEY (random secure string)")
    print("   ‚Ä¢ CRON_SECRET (random secure string)")
    
    print("\n3. Configure GitHub repository secrets:")
    print("   ‚Ä¢ VERCEL_SCRAPE_URL (your Vercel app URL)")
    print("   ‚Ä¢ CRON_SECRET (same as Vercel)")
    
    print("\n4. Push GitHub Actions workflow:")
    print("   git add .github/workflows/daily-scrape.yml")
    print("   git commit -m 'Add daily scraping automation'")
    print("   git push")
    
    print("\n5. Test the automation:")
    print("   ‚Ä¢ Go to GitHub Actions tab")
    print("   ‚Ä¢ Manually trigger the workflow")
    print("   ‚Ä¢ Verify it runs successfully")
    
    print("\n‚ö° ONCE SET UP:")
    print("‚Ä¢ Your scraper will run automatically daily at 2 AM UTC")
    print("‚Ä¢ No computer needed - runs entirely in the cloud")
    print("‚Ä¢ Data stored in your Vercel deployment")
    print("‚Ä¢ Accessible via web interface")

def main():
    """Main function"""
    print("üîç CLOUD DEPLOYMENT STATUS CHECK")
    print("="*60)
    
    check_vercel_deployment()
    check_github_actions()
    check_api_endpoints()
    check_environment_config()
    deployment_summary()

if __name__ == "__main__":
    main()
